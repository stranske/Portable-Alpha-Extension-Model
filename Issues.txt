1. Add Streamlit LLM foundations (optional deps + core helpers): |
 ## Why

 Portable Alpha‚Äôs Streamlit app should support LangChain-powered feedback (Explain Results, Compare Runs, Config Chat) with the same core patterns as Trend (provider selection, safe key handling, LangSmith trace URLs, caching). Right now Portable Alpha lacks a dedicated, Streamlit-oriented LLM foundation.

 ## Scope

 - Add an `llm` optional dependency group and lock alignment
 - Add `pa_core/llm/` foundational modules:
   - provider config + `create_llm()`
   - LangSmith tracing helper (trace_url capture)
   - shared prompt helpers and disclaimers for Streamlit features
 - Ensure all LLM features degrade gracefully when `.[llm]` is not installed or keys are missing

 ## Non-Goals

 - Implement the Streamlit UI panels (Explain/Compare/Config Chat) in this issue
 - Add CI workflows or change `.github/workflows/**` (keep this issue repo-code only)

 ## Tasks

 - [ ] Add `llm` optional dependency group in `pyproject.toml` with: `langchain`, `langchain-core`, `langchain-openai`, `langchain-anthropic`, `langsmith` (keep versions loosely pinned or consistent with Trend‚Äôs patterns)
 - [ ] Update `requirements.lock` generation command and regenerate `requirements.lock` so `tests/test_dependency_version_alignment.py` continues to pass
 - [ ] Create `pa_core/llm/tracing.py` mirroring Trend‚Äôs pattern: enable tracing only when `LANGSMITH_API_KEY` exists; expose `langsmith_tracing_context()` and `resolve_trace_url()`
 - [ ] Create `pa_core/llm/provider.py` (or similar) defining a small `LLMProviderConfig` and `create_llm(config)` that builds a LangChain chat model
 - [ ] Add `pa_core/llm/prompts.py` with prompt builders for:
   - result explanation
   - comparison explanation
   - config patching (wizard)
 - [ ] Add unit tests that:
   - confirm tracing is a no-op without env keys
   - validate `create_llm()` fails cleanly with a helpful error if provider keys missing

 ## Acceptance Criteria

 - [ ] `pip install -e ".[llm]"` succeeds locally and in CI, and `requirements.lock` includes pinned versions for new deps
 - [ ] `import pa_core.llm` works without triggering network calls; tracing remains disabled unless `LANGSMITH_API_KEY` is set
 - [ ] Unit tests pass without any real LLM calls

 ## Implementation Notes

 - Trend references to mirror (conceptually):
   - `src/trend_analysis/llm/tracing.py` (LangSmith gating + trace URL)
   - `streamlit_app/components/llm_settings.py` (sanitized key handling, env/secret fallback)
 - Prefer Portable-specific env names (ex: `PA_LLM_PROVIDER`, `PA_LLM_MODEL`, `PA_LLM_API_KEY`) but allow fallbacks (`OPENAI_API_KEY`, `CLAUDE_API_STRANSKE`) for convenience.
 - Never persist raw API keys into artifacts, logs, or exported packets.

3. Add Trend Streamlit LLM reference pack config to Portable Alpha: |
 ## Why

 Once Workflows supports reference packs, Portable Alpha needs a pinned, curated ‚ÄúTrend LLM Streamlit‚Äù pack so Codex can copy patterns reliably.

 ## Scope

 - Add `.github/reference_packs.json` in Portable Alpha defining a Trend reference pack
 - Pin to a specific Trend commit SHA
 - Include only the minimal files needed for:
   - LLM settings UI patterns
   - Explain Results UI patterns
   - Comparison UI patterns
   - Tracing helpers and chain invocation patterns
   - Config Chat patterns (even if we re-implement locally)

 ## Non-Goals

 - Implement the Streamlit features themselves
 - Include large or unrelated Trend directories

 ## Tasks

 - [ ] Create `.github/reference_packs.json` with one pack named `trend_streamlit_llm`
 - [ ] Pin `ref` to Trend `phase-3` HEAD commit (use current SHA: `8356d93c951ba39074d834875edbb7faa187da32`)
 - [ ] Include these paths (minimum viable set):
   - `streamlit_app/components/llm_settings.py`
   - `streamlit_app/components/explain_results.py`
   - `streamlit_app/components/comparison_llm.py`
   - `streamlit_app/pages/2_Model.py` (for Config Chat patterns)
   - `src/trend_analysis/llm/chain.py`
   - `src/trend_analysis/llm/tracing.py`
 - [ ] Add a short doc note in `docs/` describing what the reference pack is and how it is used in agent runs

 ## Acceptance Criteria

 - [ ] The reference pack config is valid JSON and includes a pinned commit SHA
 - [ ] After the Workflows change is merged, a keepalive Codex run in Portable Alpha contains `.reference/trend_streamlit_llm/...` with those files

 ## Implementation Notes

 - Keep the pack small. If you later need more, add paths incrementally (avoid prompt bloat and workspace noise).
 - Do not commit `.reference/**` (it should be runtime-only materialized by Workflows).

4. Streamlit LLM settings component for Portable Alpha (Trend-style): |
 ## Why

 Explain/Compare/Config Chat should share consistent handling for provider/model/key selection (and avoid leaking secrets). Trend already has a good pattern.

 ## Scope

 - Add a shared Streamlit component for LLM settings (Portable-native)
 - Support reading keys from `st.secrets` and environment, plus optional ‚Äúenv var name‚Äù inputs
 - Provide a provider config object compatible with `pa_core/llm/create_llm()`

 ## Non-Goals

 - Build Explain/Compare/Config Chat panels in this issue

 ## Tasks

 - [ ] Add `dashboard/components/llm_settings.py` modeled after Trend‚Äôs `streamlit_app/components/llm_settings.py`
 - [ ] Implement:
   - [ ] `sanitize_api_key()`
   - [ ] `read_secret()`
   - [ ] `resolve_api_key_input()` (accept either raw key or env-var name)
   - [ ] `default_api_key(provider)` (Portable defaults + fallbacks)
   - [ ] `resolve_llm_provider_config(...)`
 - [ ] Define Portable env defaults (recommended):
   - `PA_LLM_PROVIDER`, `PA_LLM_MODEL`, `PA_LLM_BASE_URL`, `PA_LLM_ORG`, `PA_STREAMLIT_API_KEY`
   - fall back to `OPENAI_API_KEY`, `CLAUDE_API_STRANSKE`, `LANGSMITH_API_KEY`
 - [ ] Add a tiny unit test suite validating sanitize/resolve behavior without printing secrets

 ## Acceptance Criteria

 - [ ] Streamlit can render the settings component without exceptions even when no keys are set
 - [ ] Entering an env var name like `OPENAI_API_KEY` resolves to the secret value (without displaying it)
 - [ ] Missing key errors are user-friendly and do not expose sensitive values

 ## Implementation Notes

 - Keep provider options aligned with Trend where possible: `openai`, `anthropic` (and optionally `ollama` if you want local)
 - Never store raw keys in exported files, manifests, or logs.

5. Add Explain Results LLM panel to dashboard/pages/4_Results.py: |
 ## Why

 Portable Alpha‚Äôs Results page already surfaces summary metrics and exports. Adding a Trend-like ‚ÄúExplain Results‚Äù panel will let users quickly interpret drivers (risk/return, breach probability, CVaR, scenario mechanics) with traceable LangSmith runs.

 ## Scope

 - Implement `dashboard/components/explain_results.py` (Streamlit UI)
 - Implement `pa_core/llm/result_explain.py` (metric extraction + prompt builder + chain invocation)
 - Wire into `dashboard/pages/4_Results.py`
 - Cache results per run, provider, model, and question text

 ## Non-Goals

 - Compare runs (separate issue)
 - Wizard Config Chat (separate issue)

 ## Tasks

 - [ ] Create `pa_core/llm/result_explain.py`:
   - [ ] Build a compact ‚Äúanalysis_output‚Äù string from:
     - summary DataFrame stats (means, tails, key quantiles)
     - available column names
     - manifest highlights (seed, key CLI args if available)
     - optional StressDelta sheet summary if present
   - [ ] Build a ‚Äúmetric_catalog‚Äù from key summary columns (TE, CVaR, breach prob, etc.)
   - [ ] Invoke LLM via `pa_core/llm/create_llm()` and return `(text, trace_url)`
   - [ ] Ensure a disclaimer/footer is appended (Trend-style)
 - [ ] Create `dashboard/components/explain_results.py`:
   - [ ] Questions text area with a sensible default tailored to Portable Alpha
   - [ ] Provider/key/model expander using `dashboard/components/llm_settings.py`
   - [ ] ‚ÄúExplain Results‚Äù button with spinner
   - [ ] Cache in `st.session_state` keyed by (xlsx path + manifest seed + provider/model/questions)
   - [ ] Download buttons for TXT + JSON payload (include trace URL, created_at, inputs summary)
 - [ ] Wire into `dashboard/pages/4_Results.py`:
   - [ ] Render the panel below Key Metrics and above exports (or inside a tab/expander)
   - [ ] Graceful fallback when llm deps not installed (show ‚Äúinstall .[llm]‚Äù hint)

 ## Acceptance Criteria

 - [ ] With `.[llm]` installed and an API key present, Results page can generate an explanation and display a trace URL when LangSmith is enabled
 - [ ] Without `.[llm]`, the page still works and shows a clear ‚ÄúLLM features unavailable‚Äù message
 - [ ] No secrets appear in the UI output, downloads, or logs

 ## Implementation Notes

 - Use Trend‚Äôs `streamlit_app/components/explain_results.py` as the structural reference.
 - Portable‚Äôs ‚Äúdetails‚Äù object is a DataFrame + manifest, not Trend‚Äôs dict, so focus on robust summarization of tabular outputs.

6. Add LLM Comparison panel (current run vs previous run): |
 ## Why

 Portable Alpha already tracks `previous_run` in the manifest and attempts to load prior outputs for exports. A Trend-like LLM comparison panel will explain why results moved between iterations.

 ## Scope

 - Implement `dashboard/components/comparison_llm.py` (Streamlit UI)
 - Implement `pa_core/llm/compare_runs.py` (diff + prompt + chain)
 - Wire into `dashboard/pages/4_Results.py`, using `manifest_data["previous_run"]` when present

 ## Non-Goals

 - Config Chat patching (separate issue)

 ## Tasks

 - [ ] Implement `pa_core/llm/compare_runs.py`:
   - [ ] Load prior manifest and prior summary when `previous_run` exists and is readable
   - [ ] Compute a human-readable diff of key config inputs:
     - seed changes
     - key CLI args (capital, weights, distribution choices)
     - any wizard config fields if stored in manifest
   - [ ] Build two metric catalogs (A/B) from both summaries (same metrics as Explain Results)
   - [ ] Invoke LLM and return `(text, trace_url)`
 - [ ] Implement `dashboard/components/comparison_llm.py`:
   - [ ] UI to select current vs previous (auto-detect previous)
   - [ ] Questions box with default tailored to Portable Alpha (drivers, risk constraints, breach, CVaR)
   - [ ] Provider/key/model controls via shared LLM settings component
   - [ ] Downloads (TXT/JSON) including config diff + trace URL
 - [ ] Wire into `dashboard/pages/4_Results.py`:
   - [ ] Show only when previous run is available; otherwise show a gentle message

 ## Acceptance Criteria

 - [ ] When a previous run exists, the comparison panel produces a coherent explanation and shows trace URL (when enabled)
 - [ ] When previous run is missing/unreadable, the Results page still loads and explains what is missing

 ## Implementation Notes

 - Use Trend‚Äôs `streamlit_app/components/comparison_llm.py` as the structural reference.
 - Reuse any prior-run loading logic already present in Results export packet code to avoid duplication.

7. Add Config Chat to Scenario Wizard (NL patch + diff preview + apply): |
 ## Why

 To match Trend‚Äôs Streamlit experience, users should be able to describe a change in plain English (‚Äúincrease simulations to 5000 and reduce breach tolerance‚Äù) and get a preview diff before applying to the wizard config.

 ## Scope

 - Implement a Wizard ‚ÄúConfig Chat‚Äù panel similar to Trend‚Äôs, but adapted to `DefaultConfigView` and session-state keys used by the wizard
 - Support preview, apply, apply+validate, and revert
 - Block unknown keys and add basic injection guard

 ## Non-Goals

 - Changing the underlying simulation math
 - Adding new wizard steps or redesigning the wizard UX

 ## Tasks

 - [ ] Create `pa_core/llm/config_patch.py`:
   - [ ] Define a patch format (set/merge/remove) that targets a known set of wizard fields
   - [ ] Implement apply + diff functions for the wizard config (and session-state mirrors like `_TOTAL_CAPITAL_KEY`, etc.)
   - [ ] Validate by round-tripping through `_build_yaml_from_config()` + `load_config()`
 - [ ] Create `pa_core/llm/config_patch_chain.py`:
   - [ ] Prompt builder that includes:
     - current wizard config snapshot
     - allowed schema (explicit list of keys and types)
     - safety rules (no secrets, no workflow edits, no file writes)
   - [ ] Return patch + summary + risk flags
   - [ ] Capture trace URL when LangSmith enabled
 - [ ] Add `dashboard/components/config_chat.py`:
   - [ ] UI: Instruction input, Preview button, Apply, Apply+Validate, Revert
   - [ ] Render unified diff and side-by-side diff (YAML or JSON)
   - [ ] Persist change history in `st.session_state` with timestamps
 - [ ] Wire into `dashboard/pages/3_Scenario_Wizard.py`:
   - [ ] Add a sidebar expander ‚Äúüí¨ Config Chat‚Äù
   - [ ] On apply, update both `wizard_config` and relevant session keys

 ## Acceptance Criteria

 - [ ] A user can enter an instruction, see a preview diff, then apply it and pass `load_config()` validation
 - [ ] Unknown keys are rejected or stripped (no silent hallucinated fields)
 - [ ] Revert restores the last pre-apply config state

 ## Implementation Notes

 - Use Trend‚Äôs config chat flow as the UX reference, but keep Portable-specific schema.
 - Keep the allowed schema explicit and small at first (core wizard fields only). Expand later as needed.

8. Add tests + docs for Streamlit LLM features and reference pack: |
 ## Why

 LLM features are easy to break with small refactors. Lightweight tests (no network) and docs keep the system reliable and reproducible for agent runs.

 ## Scope

 - Unit tests for prompt builders, metric extraction, diff/apply logic, and key resolution
 - Smoke tests for Streamlit component imports
 - Docs explaining:
   - enabling `.[llm]`
   - required env vars / secrets
   - how the reference pack works in keepalive runs

 ## Non-Goals

 - Integration tests that call real LLM endpoints

 ## Tasks

 - [ ] Add tests for:
   - [ ] `dashboard/components/llm_settings.py` key resolution and sanitization
   - [ ] `pa_core/llm/result_explain.py` metric extraction (with a small fake summary DataFrame)
   - [ ] `pa_core/llm/compare_runs.py` diff formatting
   - [ ] `pa_core/l_
